{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import gymnasium as gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialNetworkEnv(gym.Env):\n",
    "\n",
    "    #Env setup\n",
    "    def __init__(self, numConsumer = 10):\n",
    "        super().__init__()\n",
    "        agent_type = np.random.choice([\"real-information\", \"fake-information\", \"fact-checker\", \"consumer\"])\n",
    "\n",
    "        # Create Network\n",
    "        self.graph = nx.DiGraph()\n",
    "        \n",
    "        # Add consumers\n",
    "        for i in range(numConsumer):\n",
    "            self.graph.add_node(i,\n",
    "                                type=\"consumer\", \n",
    "                                Q_value=0.0, \n",
    "                                trust_level=1.0, \n",
    "                                stored_information=[], \n",
    "                                reward=0, \n",
    "                                penalty=0)\n",
    "        \n",
    "        # Each consumer connects with ~2 others\n",
    "        for _ in range(numConsumer * 2):  \n",
    "            src = np.random.randint(0, numConsumer)\n",
    "            dst = np.random.randint(0, numConsumer)\n",
    "            if src != dst:\n",
    "                self.graph.add_edge(src, dst, weight=1.0)\n",
    "\n",
    "\n",
    "        # Fake information agent connected with every consumer\n",
    "        self.graph.add_node(numConsumer,\n",
    "                            type=\"fake-information\", \n",
    "                            Q_value=0.0, \n",
    "                            reward=0, \n",
    "                            penalty=0)\n",
    "        \n",
    "        for node in self.graph.nodes:\n",
    "            if node != 0:\n",
    "                self.graph.add_edge(numConsumer, node)\n",
    "\n",
    "        \n",
    "\n",
    "        self.numConsumers = numConsumer\n",
    "        #Action Space is a binary array indicating whether the agent sends information\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(numConsumer,), dtype=np.int32)\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"trust_levels\": spaces.Box(low=0, high=1, shape=(numConsumer,), dtype=np.float32),\n",
    "            \"Q_values\": spaces.Box(low=-np.inf, high=np.inf, shape=(numConsumer,), dtype=np.float32),\n",
    "        })\n",
    "\n",
    "    #Reset Env\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Reset agents\n",
    "        for node in self.graph.nodes:\n",
    "            nodeType = self.graph.nodes[node][\"type\"]\n",
    "            if nodeType == \"consumer\":\n",
    "                self.graph.nodes[node][\"Q_value\"] = 0.0\n",
    "                self.graph.nodes[node][\"trust_level\"] = 1.0\n",
    "                self.graph.nodes[node][\"stored_information\"] = []\n",
    "                self.graph.nodes[node][\"reward\"] = 0\n",
    "                self.graph.nodes[node][\"penalty\"] = 0\n",
    "            elif nodeType == \"fake-information\":\n",
    "                self.graph.nodes[node][\"Q_value\"] = 0.0\n",
    "                self.graph.nodes[node][\"reward\"] = 0\n",
    "                self.graph.nodes[node][\"penalty\"] = 0\n",
    "\n",
    "        # Generate random trust levels for all agents\n",
    "        trust_levels = np.random.rand(self.numConsumers)\n",
    "        \n",
    "        # Return initial observation\n",
    "        return {\"trust_levels\": trust_levels, \n",
    "                \"Q_values\": np.zeros(self.numConsumers)}, {}\n",
    "        \n",
    "    def step(self, action):\n",
    "        rewards = 0\n",
    "        penalties = 0\n",
    "\n",
    "        #CHANGE LATER\n",
    "        node = self.numConsumers\n",
    "\n",
    "        for neighbor, send_info in zip(self.graph.neighbors(node), action):\n",
    "            if send_info == 1:  # Propagate news to this neighbor\n",
    "                neighbor_data = self.graph.nodes[neighbor]\n",
    "                \n",
    "                if self.graph.nodes[node][\"type\"] == \"consumer\":\n",
    "                    # Update trust-level and stored-information based on the source\n",
    "                    if self.graph.nodes[node][\"type\"] == \"fake-information\":\n",
    "                        neighbor_data[\"trust_level\"] -= 0.1\n",
    "                    elif self.graph.nodes[node][\"type\"] == \"real-information\":\n",
    "                        neighbor_data[\"trust_level\"] += 0.1\n",
    "                    \n",
    "                    neighbor_data[\"stored_information\"].append({\n",
    "                        \"news\": f\"news_from_{node}\",\n",
    "                        \"source\": node,\n",
    "                        \"truthfulness\": np.random.uniform(0, 100) if self.graph.nodes[node][\"type\"] == \"fake-information\" else np.random.uniform(50, 100)\n",
    "                    })\n",
    "                    \n",
    "                    if neighbor_data[\"type\"] == \"consumer\":\n",
    "                        rewards += 0.5  # Influence gained by consumer agents\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        max_q_value = max(rewards - penalties, 0)\n",
    "        self.graph.nodes[node][\"Q_value\"] += 0.1 * (rewards - penalties + 0.9 * max_q_value - self.graph.nodes[node][\"Q_value\"])\n",
    "    \n",
    "        # Return the updated state\n",
    "        trust_levels = np.array([self.graph.nodes[i][\"trust_level\"] for i in range(self.numConsumers)])\n",
    "        q_values = np.array([self.graph.nodes[i][\"Q_value\"] for i in range(self.numConsumers)])\n",
    "        done = False  # In this simulation, the environment does not end\n",
    "        info = {}\n",
    "\n",
    "        return {\"trust_levels\": trust_levels, \"Q_values\": q_values}, rewards, done, info\n",
    "    \n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"\n",
    "        Optional: Render the graph for debugging or visualization.\n",
    "        \"\"\"\n",
    "        if mode == \"human\":\n",
    "            print(\"Graph Nodes and Attributes:\")\n",
    "            for node, data in self.graph.nodes(data=True):\n",
    "                print(f\"Node {node}: {data}\")\n",
    "            print(\"Graph Edges:\")\n",
    "            for src, dst, data in self.graph.edges(data=True):\n",
    "                print(f\"Edge {src} -> {dst}: {data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Nodes and Attributes:\n",
      "Node 0: {'type': 'consumer', 'Q_value': 0.0, 'trust_level': 1.0, 'stored_information': [], 'reward': 0, 'penalty': 0}\n",
      "Node 1: {'type': 'consumer', 'Q_value': 0.0, 'trust_level': 1.0, 'stored_information': [], 'reward': 0, 'penalty': 0}\n",
      "Node 2: {'type': 'consumer', 'Q_value': 0.0, 'trust_level': 1.0, 'stored_information': [], 'reward': 0, 'penalty': 0}\n",
      "Node 3: {'type': 'consumer', 'Q_value': 0.0, 'trust_level': 1.0, 'stored_information': [], 'reward': 0, 'penalty': 0}\n",
      "Node 4: {'type': 'consumer', 'Q_value': 0.0, 'trust_level': 1.0, 'stored_information': [], 'reward': 0, 'penalty': 0}\n",
      "Node 5: {'type': 'consumer', 'Q_value': 0.0, 'trust_level': 1.0, 'stored_information': [], 'reward': 0, 'penalty': 0}\n",
      "Node 6: {'type': 'consumer', 'Q_value': 0.0, 'trust_level': 1.0, 'stored_information': [], 'reward': 0, 'penalty': 0}\n",
      "Node 7: {'type': 'consumer', 'Q_value': 0.0, 'trust_level': 1.0, 'stored_information': [], 'reward': 0, 'penalty': 0}\n",
      "Node 8: {'type': 'consumer', 'Q_value': 0.0, 'trust_level': 1.0, 'stored_information': [], 'reward': 0, 'penalty': 0}\n",
      "Node 9: {'type': 'consumer', 'Q_value': 0.0, 'trust_level': 1.0, 'stored_information': [], 'reward': 0, 'penalty': 0}\n",
      "Node 10: {'type': 'fake-information', 'Q_value': 0.0, 'reward': 0, 'penalty': 0}\n",
      "Graph Edges:\n",
      "Edge 0 -> 4: {'weight': 1.0}\n",
      "Edge 0 -> 5: {'weight': 1.0}\n",
      "Edge 0 -> 7: {'weight': 1.0}\n",
      "Edge 0 -> 2: {'weight': 1.0}\n",
      "Edge 2 -> 5: {'weight': 1.0}\n",
      "Edge 3 -> 4: {'weight': 1.0}\n",
      "Edge 4 -> 3: {'weight': 1.0}\n",
      "Edge 4 -> 6: {'weight': 1.0}\n",
      "Edge 5 -> 2: {'weight': 1.0}\n",
      "Edge 5 -> 4: {'weight': 1.0}\n",
      "Edge 6 -> 2: {'weight': 1.0}\n",
      "Edge 6 -> 3: {'weight': 1.0}\n",
      "Edge 7 -> 4: {'weight': 1.0}\n",
      "Edge 7 -> 9: {'weight': 1.0}\n",
      "Edge 8 -> 6: {'weight': 1.0}\n",
      "Edge 9 -> 5: {'weight': 1.0}\n",
      "Edge 9 -> 1: {'weight': 1.0}\n",
      "Edge 10 -> 1: {}\n",
      "Edge 10 -> 2: {}\n",
      "Edge 10 -> 3: {}\n",
      "Edge 10 -> 4: {}\n",
      "Edge 10 -> 5: {}\n",
      "Edge 10 -> 6: {}\n",
      "Edge 10 -> 7: {}\n",
      "Edge 10 -> 8: {}\n",
      "Edge 10 -> 9: {}\n",
      "Edge 10 -> 10: {}\n"
     ]
    }
   ],
   "source": [
    "env = SocialNetworkEnv(numConsumer=10)\n",
    "obs, _ = env.reset()\n",
    "actions = np.random.choice([0, 1], size=(env.numConsumers))  # Random actions for each agent and neighbor\n",
    "obs, rewards, done, info = env.step(actions)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation Results:\n",
      "Average Influence: 476.64\n",
      "Max Influence: 1000.00\n",
      "Min Influence: -1000.00\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gymnasium as gym\n",
    "# from gymnasium import spaces\n",
    "# import numpy as np\n",
    "# import networkx as nx\n",
    "\n",
    "\n",
    "# class SocialNetworkEnv(gym.Env):\n",
    "#     \"\"\"\n",
    "#     Custom Environment for simulating a social network with agents of different types interacting.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, num_agents=10):\n",
    "#         super(SocialNetworkEnv, self).__init__()\n",
    "        \n",
    "#         # Define the graph representing the network\n",
    "#         self.graph = nx.DiGraph()\n",
    "        \n",
    "#         # Add nodes (agents) with attributes\n",
    "#         for i in range(num_agents):\n",
    "#             agent_type = np.random.choice([\"real-information\", \"fake-information\", \"fact-checker\", \"consumer\"])\n",
    "#             self.graph.add_node(i, \n",
    "#                                 type=agent_type, \n",
    "#                                 Q_value=0.0, \n",
    "#                                 trust_level=1.0, \n",
    "#                                 stored_information=[], \n",
    "#                                 reward=0, \n",
    "#                                 penalty=0)\n",
    "        \n",
    "#         # Add random edges (connections) with initial weights\n",
    "#         for _ in range(num_agents * 2):  # Each agent connects with ~2 others\n",
    "#             src = np.random.randint(0, num_agents)\n",
    "#             dst = np.random.randint(0, num_agents)\n",
    "#             if src != dst:\n",
    "#                 self.graph.add_edge(src, dst, weight=1.0)\n",
    "        \n",
    "#         # Action and observation spaces\n",
    "#         self.num_agents = num_agents\n",
    "#         self.action_space = spaces.Discrete(2)  # Each agent can choose to propagate news (1) or not (0)\n",
    "#         self.observation_space = spaces.Dict({\n",
    "#             \"trust_levels\": spaces.Box(low=0, high=1, shape=(num_agents,), dtype=np.float32),\n",
    "#             \"Q_values\": spaces.Box(low=-np.inf, high=np.inf, shape=(num_agents,), dtype=np.float32),\n",
    "#         })\n",
    "\n",
    "#     def reset(self, seed=None, options=None):\n",
    "#         super().reset(seed=seed)\n",
    "        \n",
    "#         # Reset agents\n",
    "#         for node in self.graph.nodes:\n",
    "#             self.graph.nodes[node][\"Q_value\"] = 0.0\n",
    "#             self.graph.nodes[node][\"trust_level\"] = 1.0\n",
    "#             self.graph.nodes[node][\"stored_information\"] = []\n",
    "#             self.graph.nodes[node][\"reward\"] = 0\n",
    "#             self.graph.nodes[node][\"penalty\"] = 0\n",
    "        \n",
    "#         # Generate random trust levels for all agents\n",
    "#         trust_levels = np.random.rand(self.num_agents)\n",
    "        \n",
    "#         # Return initial observation\n",
    "#         return {\"trust_levels\": trust_levels, \n",
    "#                 \"Q_values\": np.zeros(self.num_agents)}, {}\n",
    "\n",
    "#     def step(self, actions):\n",
    "#         \"\"\"\n",
    "#         Step function simulates one iteration of agents interacting in the graph.\n",
    "#         Each agent decides whether to propagate news or not based on its Q-value and trust-level.\n",
    "#         \"\"\"\n",
    "#         rewards = np.zeros(self.num_agents)\n",
    "#         penalties = np.zeros(self.num_agents)\n",
    "        \n",
    "#         # Simulate actions for all agents\n",
    "#         for node, action in enumerate(actions):\n",
    "#             if action == 1:  # Propagate news\n",
    "#                 for neighbor in self.graph.neighbors(node):\n",
    "#                     neighbor_data = self.graph.nodes[neighbor]\n",
    "                    \n",
    "#                     # Update trust-level and stored-information based on the source\n",
    "#                     if self.graph.nodes[node][\"type\"] == \"fake-information\":\n",
    "#                         neighbor_data[\"trust_level\"] -= 0.1\n",
    "#                     elif self.graph.nodes[node][\"type\"] == \"real-information\":\n",
    "#                         neighbor_data[\"trust_level\"] += 0.1\n",
    "                    \n",
    "#                     neighbor_data[\"stored_information\"].append({\n",
    "#                         \"news\": f\"news_from_{node}\",\n",
    "#                         \"source\": node,\n",
    "#                         \"truthfulness\": np.random.uniform(0, 100) if self.graph.nodes[node][\"type\"] == \"fake-information\" else np.random.uniform(50, 100)\n",
    "#                     })\n",
    "                    \n",
    "#                     # Assign rewards/penalties\n",
    "#                     if neighbor_data[\"type\"] == \"fact-checker\":\n",
    "#                         if neighbor_data[\"stored_information\"][-1][\"truthfulness\"] < 50:\n",
    "#                             rewards[node] += 1  # Caught fake news\n",
    "#                             penalties[node] += 1  # Penalize misinformation source\n",
    "                    \n",
    "#                     if neighbor_data[\"type\"] == \"consumer\":\n",
    "#                         rewards[node] += 0.5  # Influence gained\n",
    "\n",
    "#         # Update Q-values for all nodes\n",
    "#         for node in self.graph.nodes:\n",
    "#             max_q_value = max(rewards[node] - penalties[node], 0)\n",
    "#             self.graph.nodes[node][\"Q_value\"] += 0.1 * (rewards[node] - penalties[node] + 0.9 * max_q_value - self.graph.nodes[node][\"Q_value\"])\n",
    "        \n",
    "#         # Return the updated state\n",
    "#         trust_levels = np.array([self.graph.nodes[i][\"trust_level\"] for i in range(self.num_agents)])\n",
    "#         q_values = np.array([self.graph.nodes[i][\"Q_value\"] for i in range(self.num_agents)])\n",
    "#         done = False  # In this simulation, the environment does not end\n",
    "#         info = {}\n",
    "\n",
    "#         return {\"trust_levels\": trust_levels, \"Q_values\": q_values}, rewards, done, info\n",
    "\n",
    "#     def render(self, mode=\"human\"):\n",
    "#         \"\"\"\n",
    "#         Optional: Render the graph for debugging or visualization.\n",
    "#         \"\"\"\n",
    "#         if mode == \"human\":\n",
    "#             print(\"Graph Nodes and Attributes:\")\n",
    "#             for node, data in self.graph.nodes(data=True):\n",
    "#                 print(f\"Node {node}: {data}\")\n",
    "#             print(\"Graph Edges:\")\n",
    "#             for src, dst, data in self.graph.edges(data=True):\n",
    "#                 print(f\"Edge {src} -> {dst}: {data}\")\n",
    "\n",
    "\n",
    "# env = SocialNetworkEnv(num_agents=10)\n",
    "# obs, _ = env.reset()\n",
    "# print(obs)\n",
    "# actions = np.random.choice([0, 1], size=10)  # Random actions\n",
    "# obs, rewards, done, info = env.step(actions)\n",
    "# print(obs, rewards)\n",
    "# env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trust_levels': array([0.34337057, 0.63483647, 0.47055293, 0.53561651, 0.7468535 ,\n",
      "       0.09321691, 0.03131633, 0.70775766, 0.65756737, 0.12412937]), 'Q_values': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}\n",
      "{'trust_levels': array([1. , 0.9, 0.9, 1. , 0.9, 0.9, 1. , 1. , 1. , 1. ]), 'Q_values': array([0.   , 0.   , 0.095, 0.   , 0.   , 0.   , 0.095, 0.   , 0.19 ,\n",
      "       0.   ])} [0.  0.  0.5 0.  0.  0.  0.5 0.  1.  0. ]\n",
      "Graph Nodes and Attributes:\n",
      "Node 0: {'type': 'fake-information', 'Q_value': 0.0, 'trust_level': 1.0, 'stored_information': [], 'reward': 0, 'penalty': 0}\n",
      "Node 1: {'type': 'fact-checker', 'Q_value': 0.0, 'trust_level': 0.9, 'stored_information': [{'news': 'news_from_0', 'source': 0, 'truthfulness': 84.93738182499965}, {'news': 'news_from_7', 'source': 7, 'truthfulness': 72.38982891008934}], 'reward': 0, 'penalty': 0}\n",
      "Node 2: {'type': 'fact-checker', 'Q_value': 0.095, 'trust_level': 0.9, 'stored_information': [{'news': 'news_from_4', 'source': 4, 'truthfulness': 64.12870330447413}], 'reward': 0, 'penalty': 0}\n",
      "Node 3: {'type': 'real-information', 'Q_value': 0.0, 'trust_level': 1.0, 'stored_information': [{'news': 'news_from_6', 'source': 6, 'truthfulness': 57.81137050655834}, {'news': 'news_from_8', 'source': 8, 'truthfulness': 98.09589799647597}], 'reward': 0, 'penalty': 0}\n",
      "Node 4: {'type': 'fake-information', 'Q_value': 0.0, 'trust_level': 0.9, 'stored_information': [{'news': 'news_from_0', 'source': 0, 'truthfulness': 5.04466016583458}, {'news': 'news_from_7', 'source': 7, 'truthfulness': 79.95903036891093}, {'news': 'news_from_8', 'source': 8, 'truthfulness': 53.24296374055885}], 'reward': 0, 'penalty': 0}\n",
      "Node 5: {'type': 'fake-information', 'Q_value': 0.0, 'trust_level': 0.9, 'stored_information': [{'news': 'news_from_0', 'source': 0, 'truthfulness': 13.356856874288903}], 'reward': 0, 'penalty': 0}\n",
      "Node 6: {'type': 'consumer', 'Q_value': 0.095, 'trust_level': 1.0, 'stored_information': [], 'reward': 0, 'penalty': 0}\n",
      "Node 7: {'type': 'consumer', 'Q_value': 0.0, 'trust_level': 1.0, 'stored_information': [{'news': 'news_from_6', 'source': 6, 'truthfulness': 76.98880332394975}, {'news': 'news_from_8', 'source': 8, 'truthfulness': 58.181678775805096}], 'reward': 0, 'penalty': 0}\n",
      "Node 8: {'type': 'consumer', 'Q_value': 0.19, 'trust_level': 1.0, 'stored_information': [], 'reward': 0, 'penalty': 0}\n",
      "Node 9: {'type': 'consumer', 'Q_value': 0.0, 'trust_level': 1.0, 'stored_information': [{'news': 'news_from_2', 'source': 2, 'truthfulness': 71.76194270493613}, {'news': 'news_from_8', 'source': 8, 'truthfulness': 72.82674464103594}], 'reward': 0, 'penalty': 0}\n",
      "Graph Edges:\n",
      "Edge 0 -> 1: {'weight': 1.0}\n",
      "Edge 0 -> 4: {'weight': 1.0}\n",
      "Edge 0 -> 5: {'weight': 1.0}\n",
      "Edge 1 -> 6: {'weight': 1.0}\n",
      "Edge 2 -> 9: {'weight': 1.0}\n",
      "Edge 4 -> 2: {'weight': 1.0}\n",
      "Edge 6 -> 7: {'weight': 1.0}\n",
      "Edge 6 -> 3: {'weight': 1.0}\n",
      "Edge 7 -> 4: {'weight': 1.0}\n",
      "Edge 7 -> 1: {'weight': 1.0}\n",
      "Edge 8 -> 4: {'weight': 1.0}\n",
      "Edge 8 -> 7: {'weight': 1.0}\n",
      "Edge 8 -> 3: {'weight': 1.0}\n",
      "Edge 8 -> 9: {'weight': 1.0}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
