{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import gymnasium as gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialNetworkEnv(gym.Env):\n",
    "\n",
    "    #Env setup\n",
    "    def __init__(self, numConsumer = 10):\n",
    "        super().__init__()\n",
    "        agent_type = np.random.choice([\"real-information\", \"fake-information\", \"fact-checker\", \"consumer\"])\n",
    "\n",
    "        # Create Network\n",
    "        self.graph = nx.DiGraph()\n",
    "        \n",
    "        # Add consumers\n",
    "        for i in range(numConsumer):\n",
    "            self.graph.add_node(i,\n",
    "                                type=\"consumer\",\n",
    "                                trustLevel=0.0, \n",
    "                                storedInfo=[], \n",
    "                                reward=0, \n",
    "                                penalty=0)\n",
    "        \n",
    "        # Each consumer connects with ~2 others\n",
    "        for _ in range(numConsumer * 2):  \n",
    "            src = np.random.randint(0, numConsumer)\n",
    "            dst = np.random.randint(0, numConsumer)\n",
    "            if src != dst:\n",
    "                self.graph.add_edge(src, dst, weight=1.0)\n",
    "\n",
    "\n",
    "        # Fake information agent connected with every consumer\n",
    "        self.graph.add_node(numConsumer,\n",
    "                            type=\"fake-information\", \n",
    "                            qVal=0.0, \n",
    "                            reward=0, \n",
    "                            penalty=0)\n",
    "        \n",
    "        for node in self.graph.nodes:\n",
    "            if node != numConsumer:\n",
    "                self.graph.add_edge(numConsumer, node)\n",
    "\n",
    "        \n",
    "\n",
    "        self.numConsumers = numConsumer\n",
    "        #Action Space is a binary array indicating whether the agent sends information\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(numConsumer,), dtype=np.int32)\n",
    "        print(self.action_space)\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"trustLevels\": spaces.Box(low=0, high=1, shape=(numConsumer,), dtype=np.float32),\n",
    "        })\n",
    "\n",
    "\n",
    "    #Reset Env\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Reset agents\n",
    "        for node in self.graph.nodes:\n",
    "            nodeType = self.graph.nodes[node][\"type\"]\n",
    "            if nodeType == \"consumer\":\n",
    "                self.graph.nodes[node][\"trustLevel\"] = 0.0\n",
    "                self.graph.nodes[node][\"storedInfo\"] = []\n",
    "                self.graph.nodes[node][\"reward\"] = 0\n",
    "                self.graph.nodes[node][\"penalty\"] = 0\n",
    "            elif nodeType == \"fake-information\":\n",
    "                self.graph.nodes[node][\"qVal\"] = 0.0\n",
    "                self.graph.nodes[node][\"reward\"] = 0\n",
    "                self.graph.nodes[node][\"penalty\"] = 0\n",
    "\n",
    "        # Generate random trust levels for all agents\n",
    "        trustLevels = np.array([self.graph.nodes[i][\"trustLevel\"] for i in range(self.numConsumers)])\n",
    "        \n",
    "        # Return initial observation\n",
    "        return {\"trustLevels\": trustLevels}, {}\n",
    "        \n",
    "\n",
    "    #Step Function for Env\n",
    "    #NEED TO FIX\n",
    "    def step(self, action, agent = 10):\n",
    "        print(action)\n",
    "        rewards = 0\n",
    "        penalties = 0\n",
    "\n",
    "        #CHANGE LATER\n",
    "        actionNode = self.graph.nodes[agent]\n",
    "\n",
    "        visited = set()\n",
    "        queue = []\n",
    "\n",
    "        #Goes through and spreads news from source\n",
    "        for neighbor, sendInfo in zip(self.graph.neighbors(agent), action):\n",
    "            if sendInfo == 1:\n",
    "                queue.append(neighbor)\n",
    "\n",
    "        while queue:\n",
    "            curVal = queue.pop()\n",
    "            if curVal in visited:\n",
    "                break\n",
    "\n",
    "            visited.add(curVal)\n",
    "            curNode = self.graph.nodes[curVal]\n",
    "\n",
    "            if curNode[\"type\"] == \"consumer\":\n",
    "                # Update trust-level and stored-information based on the source\n",
    "                if actionNode[\"type\"] == \"fake-information\" and np.random.random() > 1/(1 + math.exp(-curNode[\"trustLevel\"])):\n",
    "                    curNode[\"trustLevel\"] -= 0.1\n",
    "                    rewards += 1\n",
    "\n",
    "                    for neighbor in self.graph.neighbors(curVal):\n",
    "                        queue.append(neighbor)\n",
    "                elif actionNode[\"type\"] == \"real-information\" and np.random.random() < 1/(1 + math.exp(-curNode[\"trustLevel\"])):\n",
    "                    curNode[\"trustLevel\"] += 0.1\n",
    "                    rewards += 1\n",
    "\n",
    "                    for neighbor in self.graph.neighbors(curVal):\n",
    "                        queue.append(neighbor)\n",
    "\n",
    "\n",
    "        #Calculates Rewards/Penalties\n",
    "        #CHANGE LATER\n",
    "        max_qVal = max(rewards - penalties, 0)\n",
    "        self.graph.nodes[agent][\"qVal\"] += 0.1 * (rewards - penalties + 0.9 * max_qVal - self.graph.nodes[agent][\"qVal\"])\n",
    "    \n",
    "        # Return the updated state\n",
    "        trustLevels = np.array([self.graph.nodes[i][\"trustLevel\"] for i in range(self.numConsumers)])\n",
    "        done = False  # In this simulation, the environment does not end\n",
    "        info = {}\n",
    "\n",
    "        return {\"trustLevels\": trustLevels}, rewards, done, info\n",
    "    \n",
    "    \n",
    "    #Render the graph for debugging or visualization\n",
    "    #NEED FIX\n",
    "    def render(self, mode=\"human\"):\n",
    "        if mode == \"human\":\n",
    "            print(\"Graph Nodes and Attributes:\")\n",
    "            for node, data in self.graph.nodes(data=True):\n",
    "                print(f\"Node {node}: {data}\")\n",
    "            print(\"Graph Edges:\")\n",
    "            for src, dst, data in self.graph.edges(data=True):\n",
    "                print(f\"Edge {src} -> {dst}: {data}\")\n",
    "\n",
    "\n",
    "            pos = nx.spring_layout(self.graph)\n",
    "            node_colors = [\n",
    "                    \"blue\" if self.graph.nodes[node][\"type\"] == \"real-information\"\n",
    "                    else \"red\" if self.graph.nodes[node][\"type\"] == \"fake-information\"\n",
    "                    else \"green\" if self.graph.nodes[node][\"type\"] == \"fact-checker\"\n",
    "                    else \"gray\"\n",
    "                    for node in self.graph.nodes\n",
    "                ]\n",
    "            \n",
    "            plt.figure(figsize=(8,8))\n",
    "            nx.draw_networkx_nodes(self.graph, pos, node_color=node_colors, node_size=500, alpha=0.8)\n",
    "            nx.draw_networkx_edges(self.graph, pos, alpha=0.5, arrows=True)\n",
    "            # nx.draw(self.graph, layout=nx.spring_layout(self.graph))\n",
    "\n",
    "            plt.title(\"Social Network Graph\", fontsize=14)\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#News Spread Agent\n",
    "#Doesn't Work\n",
    "class newsAgent:\n",
    "    def __init__(self, env: gym.Env, learning=0.1, discount=0.9, epsilon=0.1, epsilonDecay = 0.0001):\n",
    "        self.env = env\n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "        self.learning = learning  # Learning rate\n",
    "        self.discount = discount  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.epsilonDecay = epsilonDecay\n",
    "        print(np.zeros(env.action_space))\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select an action using epsilon-greedy policy.\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            print(self.q_values)\n",
    "            return int(np.argmax(self.q_values[state]))\n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state):\n",
    "        \"\"\"Update the Q-value using the Q-learning update rule.\"\"\"\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount * self.q_table[next_state, best_next_action]\n",
    "        td_error = td_target - self.q_table[state, action]\n",
    "        self.q_table[state, action] += self.learning * td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class newsAgent:\n",
    "#     def __init__(self, num_nodes, state_space_size, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "#         self.num_nodes = num_nodes  # Number of nodes the agent interacts with\n",
    "#         self.q_table = np.zeros((state_space_size, 2))  # Two actions: send (1), don't send (0)\n",
    "#         self.alpha = alpha  # Learning rate\n",
    "#         self.gamma = gamma  # Discount factor\n",
    "#         self.epsilon = epsilon  # Exploration rate\n",
    "#         print(self.num_nodes)\n",
    "#         print(self.q_table)\n",
    "\n",
    "#     def select_action(self, state):\n",
    "#         \"\"\"Select an action using epsilon-greedy policy.\"\"\"\n",
    "#         if np.random.random() < self.epsilon:  # Exploration\n",
    "#             return np.random.choice([0, 1])\n",
    "#         else:  # Exploitation\n",
    "#             return np.argmax(self.q_table[state])\n",
    "\n",
    "#     def update_q_value(self, state, action, reward, next_state):\n",
    "#         \"\"\"Update the Q-value using the Q-learning update rule.\"\"\"\n",
    "#         best_next_action = np.argmax(self.q_table[next_state])\n",
    "#         td_target = reward + self.gamma * self.q_table[next_state, best_next_action]\n",
    "#         td_error = td_target - self.q_table[state, action]\n",
    "#         self.q_table[state, action] += self.alpha * td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 1, (10,), int32)\n",
      "{'trustLevels': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}\n",
      "[1 0 0 1 1 0 0 0 0 1]\n",
      "{'trustLevels': array([ 0. ,  0. ,  0. ,  0. , -0.1,  0. ,  0. ,  0. ,  0. , -0.1])} 2 False {}\n"
     ]
    }
   ],
   "source": [
    "env = SocialNetworkEnv(numConsumer=10)\n",
    "# agent = newsAgent(env)\n",
    "obs, _ = env.reset()\n",
    "print(obs)\n",
    "for _ in range(1):\n",
    "    actions = np.random.choice([0, 1], size=(env.numConsumers))  # Random actions for each agent and neighbor\n",
    "    # actions = agent.select_action(obs)\n",
    "    # print(actions)\n",
    "    obs, rewards, done, info = env.step(actions)\n",
    "print(obs, rewards, done, info)\n",
    "# env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gymnasium as gym\n",
    "# from gymnasium import spaces\n",
    "# import numpy as np\n",
    "# import networkx as nx\n",
    "\n",
    "\n",
    "# class SocialNetworkEnv(gym.Env):\n",
    "#     \"\"\"\n",
    "#     Custom Environment for simulating a social network with agents of different types interacting.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, num_agents=10):\n",
    "#         super(SocialNetworkEnv, self).__init__()\n",
    "        \n",
    "#         # Define the graph representing the network\n",
    "#         self.graph = nx.DiGraph()\n",
    "        \n",
    "#         # Add nodes (agents) with attributes\n",
    "#         for i in range(num_agents):\n",
    "#             agent_type = np.random.choice([\"real-information\", \"fake-information\", \"fact-checker\", \"consumer\"])\n",
    "#             self.graph.add_node(i, \n",
    "#                                 type=agent_type, \n",
    "#                                 qVal=0.0, \n",
    "#                                 trustLevel=1.0, \n",
    "#                                 storedInfo=[], \n",
    "#                                 reward=0, \n",
    "#                                 penalty=0)\n",
    "        \n",
    "#         # Add random edges (connections) with initial weights\n",
    "#         for _ in range(num_agents * 2):  # Each agent connects with ~2 others\n",
    "#             src = np.random.randint(0, num_agents)\n",
    "#             dst = np.random.randint(0, num_agents)\n",
    "#             if src != dst:\n",
    "#                 self.graph.add_edge(src, dst, weight=1.0)\n",
    "        \n",
    "#         # Action and observation spaces\n",
    "#         self.num_agents = num_agents\n",
    "#         self.action_space = spaces.Discrete(2)  # Each agent can choose to propagate news (1) or not (0)\n",
    "#         self.observation_space = spaces.Dict({\n",
    "#             \"trustLevels\": spaces.Box(low=0, high=1, shape=(num_agents,), dtype=np.float32),\n",
    "#             \"qVals\": spaces.Box(low=-np.inf, high=np.inf, shape=(num_agents,), dtype=np.float32),\n",
    "#         })\n",
    "\n",
    "#     def reset(self, seed=None, options=None):\n",
    "#         super().reset(seed=seed)\n",
    "        \n",
    "#         # Reset agents\n",
    "#         for node in self.graph.nodes:\n",
    "#             self.graph.nodes[node][\"qVal\"] = 0.0\n",
    "#             self.graph.nodes[node][\"trustLevel\"] = 1.0\n",
    "#             self.graph.nodes[node][\"storedInfo\"] = []\n",
    "#             self.graph.nodes[node][\"reward\"] = 0\n",
    "#             self.graph.nodes[node][\"penalty\"] = 0\n",
    "        \n",
    "#         # Generate random trust levels for all agents\n",
    "#         trustLevels = np.random.rand(self.num_agents)\n",
    "        \n",
    "#         # Return initial observation\n",
    "#         return {\"trustLevels\": trustLevels, \n",
    "#                 \"qVals\": np.zeros(self.num_agents)}, {}\n",
    "\n",
    "#     def step(self, actions):\n",
    "#         \"\"\"\n",
    "#         Step function simulates one iteration of agents interacting in the graph.\n",
    "#         Each agent decides whether to propagate news or not based on its Q-value and trust-level.\n",
    "#         \"\"\"\n",
    "#         rewards = np.zeros(self.num_agents)\n",
    "#         penalties = np.zeros(self.num_agents)\n",
    "        \n",
    "#         # Simulate actions for all agents\n",
    "#         for node, action in enumerate(actions):\n",
    "#             if action == 1:  # Propagate news\n",
    "#                 for neighbor in self.graph.neighbors(node):\n",
    "#                     neighbor_data = self.graph.nodes[neighbor]\n",
    "                    \n",
    "#                     # Update trust-level and stored-information based on the source\n",
    "#                     if self.graph.nodes[node][\"type\"] == \"fake-information\":\n",
    "#                         neighbor_data[\"trustLevel\"] -= 0.1\n",
    "#                     elif self.graph.nodes[node][\"type\"] == \"real-information\":\n",
    "#                         neighbor_data[\"trustLevel\"] += 0.1\n",
    "                    \n",
    "#                     neighbor_data[\"storedInfo\"].append({\n",
    "#                         \"news\": f\"news_from_{node}\",\n",
    "#                         \"source\": node,\n",
    "#                         \"truthfulness\": np.random.uniform(0, 100) if self.graph.nodes[node][\"type\"] == \"fake-information\" else np.random.uniform(50, 100)\n",
    "#                     })\n",
    "                    \n",
    "#                     # Assign rewards/penalties\n",
    "#                     if neighbor_data[\"type\"] == \"fact-checker\":\n",
    "#                         if neighbor_data[\"storedInfo\"][-1][\"truthfulness\"] < 50:\n",
    "#                             rewards[node] += 1  # Caught fake news\n",
    "#                             penalties[node] += 1  # Penalize misinformation source\n",
    "                    \n",
    "#                     if neighbor_data[\"type\"] == \"consumer\":\n",
    "#                         rewards[node] += 0.5  # Influence gained\n",
    "\n",
    "#         # Update Q-values for all nodes\n",
    "#         for node in self.graph.nodes:\n",
    "#             max_qVal = max(rewards[node] - penalties[node], 0)\n",
    "#             self.graph.nodes[node][\"qVal\"] += 0.1 * (rewards[node] - penalties[node] + 0.9 * max_qVal - self.graph.nodes[node][\"qVal\"])\n",
    "        \n",
    "#         # Return the updated state\n",
    "#         trustLevels = np.array([self.graph.nodes[i][\"trustLevel\"] for i in range(self.num_agents)])\n",
    "#         qVals = np.array([self.graph.nodes[i][\"qVal\"] for i in range(self.num_agents)])\n",
    "#         done = False  # In this simulation, the environment does not end\n",
    "#         info = {}\n",
    "\n",
    "#         return {\"trustLevels\": trustLevels, \"qVals\": qVals}, rewards, done, info\n",
    "\n",
    "#     def render(self, mode=\"human\"):\n",
    "#         \"\"\"\n",
    "#         Optional: Render the graph for debugging or visualization.\n",
    "#         \"\"\"\n",
    "#         if mode == \"human\":\n",
    "#             print(\"Graph Nodes and Attributes:\")\n",
    "#             for node, data in self.graph.nodes(data=True):\n",
    "#                 print(f\"Node {node}: {data}\")\n",
    "#             print(\"Graph Edges:\")\n",
    "#             for src, dst, data in self.graph.edges(data=True):\n",
    "#                 print(f\"Edge {src} -> {dst}: {data}\")\n",
    "\n",
    "\n",
    "# env = SocialNetworkEnv(num_agents=10)\n",
    "# obs, _ = env.reset()\n",
    "# print(obs)\n",
    "# actions = np.random.choice([0, 1], size=10)  # Random actions\n",
    "# obs, rewards, done, info = env.step(actions)\n",
    "# print(obs, rewards)\n",
    "# env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "# import numpy as np\n",
    "# import random\n",
    "\n",
    "# class Agent:\n",
    "#     def __init__(self, agent_type, trustLevel=0.5):\n",
    "#         self.type = agent_type\n",
    "#         self.trustLevel = trustLevel\n",
    "#         self.qVals = {}  # Q-values: state-action pairs\n",
    "#         self.history = []   # Store information received and actions taken\n",
    "#         self.influence = 0  # Influence score for tracking performance\n",
    "\n",
    "#     def choose_action(self, neighbors, epsilon=0.1):\n",
    "#         if random.uniform(0, 1) < epsilon or not neighbors:\n",
    "#             return random.choice(neighbors) if neighbors else None  # Explore\n",
    "#         state = tuple(neighbors)\n",
    "#         actions = self.qVals.get(state, {neighbor: 0 for neighbor in neighbors})\n",
    "#         return max(actions, key=actions.get)  # Exploit\n",
    "\n",
    "#     def update_qVal(self, state, action, reward, next_state, alpha=0.1, gamma=0.9):\n",
    "#         max_next_q = max(self.qVals.get(next_state, {}).values(), default=0)\n",
    "#         current_q = self.qVals.get(state, {}).get(action, 0)\n",
    "#         self.qVals.setdefault(state, {})[action] = current_q + alpha * (reward + gamma * max_next_q - current_q)\n",
    "\n",
    "\n",
    "# class SocialNetwork:\n",
    "#     def __init__(self, num_agents, agent_distribution):\n",
    "#         self.graph = nx.Graph()\n",
    "#         self.agents = []\n",
    "\n",
    "#         for i in range(num_agents):\n",
    "#             agent_type = random.choices(list(agent_distribution.keys()), weights=list(agent_distribution.values()))[0]\n",
    "#             agent = Agent(agent_type)\n",
    "#             self.graph.add_node(i, agent=agent)\n",
    "#             self.agents.append(agent)\n",
    "\n",
    "#         # Randomly connect agents\n",
    "#         for _ in range(num_agents * 2):  # Adjust for graph density\n",
    "#             a, b = random.sample(range(num_agents), 2)\n",
    "#             if not self.graph.has_edge(a, b):\n",
    "#                 self.graph.add_edge(a, b, weight=random.uniform(0.1, 1.0))\n",
    "\n",
    "#     def run_iteration(self, epsilon=0.1, alpha=0.1, gamma=0.9):\n",
    "#         for node, data in self.graph.nodes(data=True):\n",
    "#             agent = data['agent']\n",
    "#             neighbors = list(self.graph.neighbors(node))\n",
    "#             if not neighbors:\n",
    "#                 continue\n",
    "\n",
    "#             state = tuple(neighbors)\n",
    "#             action = agent.choose_action(neighbors, epsilon)\n",
    "\n",
    "#             # Simulate the action\n",
    "#             reward, next_state = self.simulate_action(node, action)\n",
    "#             agent.update_qVal(state, action, reward, next_state, alpha, gamma)\n",
    "\n",
    "#     def simulate_action(self, node, action):\n",
    "#         agent = self.graph.nodes[node]['agent']\n",
    "#         target_agent = self.graph.nodes[action]['agent']\n",
    "\n",
    "#         # Determine rewards based on agent types\n",
    "#         if agent.type == \"fake-information\":\n",
    "#             if target_agent.type == \"fact-checker\":\n",
    "#                 # Fake-information agent penalized if caught\n",
    "#                 reward = -1 if random.random() < 0.5 else 1\n",
    "#             else:\n",
    "#                 reward = 1  # Influence increased if news spreads\n",
    "#             agent.influence += reward\n",
    "\n",
    "#         elif agent.type == \"real-information\":\n",
    "#             reward = 1  # Rewarded for spreading truthful information\n",
    "#             agent.influence += reward\n",
    "\n",
    "#         elif agent.type == \"fact-checker\":\n",
    "#             if target_agent.type == \"fake-information\":\n",
    "#                 reward = 1  # Reward for catching fake news\n",
    "#             else:\n",
    "#                 reward = -1  # Penalized for mislabeling\n",
    "#             agent.influence += reward\n",
    "\n",
    "#         else:  # Regular agents\n",
    "#             reward = 0  # No reward or penalty for regular agents\n",
    "\n",
    "#         # Update target agent trust level\n",
    "#         trust_delta = 0.1 if reward > 0 else -0.1\n",
    "#         target_agent.trustLevel = max(0, min(1, target_agent.trustLevel + trust_delta))\n",
    "\n",
    "#         # New state is the updated neighbors list\n",
    "#         next_state = tuple(self.graph.neighbors(node))\n",
    "#         return reward, next_state\n",
    "\n",
    "#     def evaluate(self):\n",
    "#         influence_scores = [agent.influence for agent in self.agents]\n",
    "#         return {\n",
    "#             \"average_influence\": np.mean(influence_scores),\n",
    "#             \"max_influence\": max(influence_scores),\n",
    "#             \"min_influence\": min(influence_scores),\n",
    "#         }\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# num_agents = 100\n",
    "# agent_distribution = {\n",
    "#     \"real-information\": 0.3,\n",
    "#     \"fake-information\": 0.3,\n",
    "#     \"fact-checker\": 0.2,\n",
    "#     \"regular\": 0.2,\n",
    "# }\n",
    "\n",
    "# network = SocialNetwork(num_agents, agent_distribution)\n",
    "\n",
    "# # Run 100 iterations\n",
    "# for iteration in range(1000):\n",
    "#     network.run_iteration()\n",
    "\n",
    "# # Evaluate results\n",
    "# results = network.evaluate()\n",
    "# print(\"Simulation Results:\")\n",
    "# print(f\"Average Influence: {results['average_influence']:.2f}\")\n",
    "# print(f\"Max Influence: {results['max_influence']:.2f}\")\n",
    "# print(f\"Min Influence: {results['min_influence']:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
