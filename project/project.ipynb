{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 156 (socialnetwork.py, line 158)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/Documents/final/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[1], line 2\u001b[0m\n    from network.socialnetwork import SocialNetworkEnv\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/Documents/final/project/network/__init__.py:1\u001b[0;36m\n\u001b[0;31m    from .socialnetwork import SocialNetworkEnv\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/Documents/final/project/network/socialnetwork.py:158\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(\"Graph Edges:\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'for' statement on line 156\n"
     ]
    }
   ],
   "source": [
    "from agents.news_agent import NewsAgent\n",
    "from network.socialnetwork import SocialNetworkEnv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SocialNetworkEnv(numConsumer=10)\n",
    "newsAgent = NewsAgent(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class newsAgent:\n",
    "#     def __init__(self, num_nodes, state_space_size, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "#         self.num_nodes = num_nodes  # Number of nodes the agent interacts with\n",
    "#         self.q_table = np.zeros((state_space_size, 2))  # Two actions: send (1), don't send (0)\n",
    "#         self.alpha = alpha  # Learning rate\n",
    "#         self.gamma = gamma  # Discount factor\n",
    "#         self.epsilon = epsilon  # Exploration rate\n",
    "#         print(self.num_nodes)\n",
    "#         print(self.q_table)\n",
    "\n",
    "#     def select_action(self, state):\n",
    "#         \"\"\"Select an action using epsilon-greedy policy.\"\"\"\n",
    "#         if np.random.random() < self.epsilon:  # Exploration\n",
    "#             return np.random.choice([0, 1])\n",
    "#         else:  # Exploitation\n",
    "#             return np.argmax(self.q_table[state])\n",
    "\n",
    "#     def update_q_value(self, state, action, reward, next_state):\n",
    "#         \"\"\"Update the Q-value using the Q-learning update rule.\"\"\"\n",
    "#         best_next_action = np.argmax(self.q_table[next_state])\n",
    "#         td_target = reward + self.gamma * self.q_table[next_state, best_next_action]\n",
    "#         td_error = td_target - self.q_table[state, action]\n",
    "#         self.q_table[state, action] += self.alpha * td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = SocialNetworkEnv(numConsumer=10)\n",
    "# agent = newsAgent(env)\n",
    "obs, _ = env.reset()\n",
    "print(f\" obs: {obs}\")\n",
    "for _ in range(1):\n",
    "    actions = np.random.choice([0, 1], size=(env.numConsumers))  # Random actions for each agent and neighbor\n",
    "    # actions = agent.select_action(obs)\n",
    "    # print(actions)\n",
    "    obs, rewards, done, info = env.step(actions)\n",
    "print(obs, rewards, done, info)\n",
    "# env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gymnasium as gym\n",
    "# from gymnasium import spaces\n",
    "# import numpy as np\n",
    "# import networkx as nx\n",
    "\n",
    "\n",
    "# class SocialNetworkEnv(gym.Env):\n",
    "#     \"\"\"\n",
    "#     Custom Environment for simulating a social network with agents of different types interacting.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, num_agents=10):\n",
    "#         super(SocialNetworkEnv, self).__init__()\n",
    "        \n",
    "#         # Define the graph representing the network\n",
    "#         self.graph = nx.DiGraph()\n",
    "        \n",
    "#         # Add nodes (agents) with attributes\n",
    "#         for i in range(num_agents):\n",
    "#             agent_type = np.random.choice([\"real-information\", \"fake-information\", \"fact-checker\", \"consumer\"])\n",
    "#             self.graph.add_node(i, \n",
    "#                                 type=agent_type, \n",
    "#                                 qVal=0.0, \n",
    "#                                 trustLevel=1.0, \n",
    "#                                 storedInfo=[], \n",
    "#                                 reward=0, \n",
    "#                                 penalty=0)\n",
    "        \n",
    "#         # Add random edges (connections) with initial weights\n",
    "#         for _ in range(num_agents * 2):  # Each agent connects with ~2 others\n",
    "#             src = np.random.randint(0, num_agents)\n",
    "#             dst = np.random.randint(0, num_agents)\n",
    "#             if src != dst:\n",
    "#                 self.graph.add_edge(src, dst, weight=1.0)\n",
    "        \n",
    "#         # Action and observation spaces\n",
    "#         self.num_agents = num_agents\n",
    "#         self.action_space = spaces.Discrete(2)  # Each agent can choose to propagate news (1) or not (0)\n",
    "#         self.observation_space = spaces.Dict({\n",
    "#             \"trustLevels\": spaces.Box(low=0, high=1, shape=(num_agents,), dtype=np.float32),\n",
    "#             \"qVals\": spaces.Box(low=-np.inf, high=np.inf, shape=(num_agents,), dtype=np.float32),\n",
    "#         })\n",
    "\n",
    "#     def reset(self, seed=None, options=None):\n",
    "#         super().reset(seed=seed)\n",
    "        \n",
    "#         # Reset agents\n",
    "#         for node in self.graph.nodes:\n",
    "#             self.graph.nodes[node][\"qVal\"] = 0.0\n",
    "#             self.graph.nodes[node][\"trustLevel\"] = 1.0\n",
    "#             self.graph.nodes[node][\"storedInfo\"] = []\n",
    "#             self.graph.nodes[node][\"reward\"] = 0\n",
    "#             self.graph.nodes[node][\"penalty\"] = 0\n",
    "        \n",
    "#         # Generate random trust levels for all agents\n",
    "#         trustLevels = np.random.rand(self.num_agents)\n",
    "        \n",
    "#         # Return initial observation\n",
    "#         return {\"trustLevels\": trustLevels, \n",
    "#                 \"qVals\": np.zeros(self.num_agents)}, {}\n",
    "\n",
    "#     def step(self, actions):\n",
    "#         \"\"\"\n",
    "#         Step function simulates one iteration of agents interacting in the graph.\n",
    "#         Each agent decides whether to propagate news or not based on its Q-value and trust-level.\n",
    "#         \"\"\"\n",
    "#         rewards = np.zeros(self.num_agents)\n",
    "#         penalties = np.zeros(self.num_agents)\n",
    "        \n",
    "#         # Simulate actions for all agents\n",
    "#         for node, action in enumerate(actions):\n",
    "#             if action == 1:  # Propagate news\n",
    "#                 for neighbor in self.graph.neighbors(node):\n",
    "#                     neighbor_data = self.graph.nodes[neighbor]\n",
    "                    \n",
    "#                     # Update trust-level and stored-information based on the source\n",
    "#                     if self.graph.nodes[node][\"type\"] == \"fake-information\":\n",
    "#                         neighbor_data[\"trustLevel\"] -= 0.1\n",
    "#                     elif self.graph.nodes[node][\"type\"] == \"real-information\":\n",
    "#                         neighbor_data[\"trustLevel\"] += 0.1\n",
    "                    \n",
    "#                     neighbor_data[\"storedInfo\"].append({\n",
    "#                         \"news\": f\"news_from_{node}\",\n",
    "#                         \"source\": node,\n",
    "#                         \"truthfulness\": np.random.uniform(0, 100) if self.graph.nodes[node][\"type\"] == \"fake-information\" else np.random.uniform(50, 100)\n",
    "#                     })\n",
    "                    \n",
    "#                     # Assign rewards/penalties\n",
    "#                     if neighbor_data[\"type\"] == \"fact-checker\":\n",
    "#                         if neighbor_data[\"storedInfo\"][-1][\"truthfulness\"] < 50:\n",
    "#                             rewards[node] += 1  # Caught fake news\n",
    "#                             penalties[node] += 1  # Penalize misinformation source\n",
    "                    \n",
    "#                     if neighbor_data[\"type\"] == \"consumer\":\n",
    "#                         rewards[node] += 0.5  # Influence gained\n",
    "\n",
    "#         # Update Q-values for all nodes\n",
    "#         for node in self.graph.nodes:\n",
    "#             max_qVal = max(rewards[node] - penalties[node], 0)\n",
    "#             self.graph.nodes[node][\"qVal\"] += 0.1 * (rewards[node] - penalties[node] + 0.9 * max_qVal - self.graph.nodes[node][\"qVal\"])\n",
    "        \n",
    "#         # Return the updated state\n",
    "#         trustLevels = np.array([self.graph.nodes[i][\"trustLevel\"] for i in range(self.num_agents)])\n",
    "#         qVals = np.array([self.graph.nodes[i][\"qVal\"] for i in range(self.num_agents)])\n",
    "#         done = False  # In this simulation, the environment does not end\n",
    "#         info = {}\n",
    "\n",
    "#         return {\"trustLevels\": trustLevels, \"qVals\": qVals}, rewards, done, info\n",
    "\n",
    "#     def render(self, mode=\"human\"):\n",
    "#         \"\"\"\n",
    "#         Optional: Render the graph for debugging or visualization.\n",
    "#         \"\"\"\n",
    "#         if mode == \"human\":\n",
    "#             print(\"Graph Nodes and Attributes:\")\n",
    "#             for node, data in self.graph.nodes(data=True):\n",
    "#                 print(f\"Node {node}: {data}\")\n",
    "#             print(\"Graph Edges:\")\n",
    "#             for src, dst, data in self.graph.edges(data=True):\n",
    "#                 print(f\"Edge {src} -> {dst}: {data}\")\n",
    "\n",
    "\n",
    "# env = SocialNetworkEnv(num_agents=10)\n",
    "# obs, _ = env.reset()\n",
    "# print(obs)\n",
    "# actions = np.random.choice([0, 1], size=10)  # Random actions\n",
    "# obs, rewards, done, info = env.step(actions)\n",
    "# print(obs, rewards)\n",
    "# env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "# import numpy as np\n",
    "# import random\n",
    "\n",
    "# class Agent:\n",
    "#     def __init__(self, agent_type, trustLevel=0.5):\n",
    "#         self.type = agent_type\n",
    "#         self.trustLevel = trustLevel\n",
    "#         self.qVals = {}  # Q-values: state-action pairs\n",
    "#         self.history = []   # Store information received and actions taken\n",
    "#         self.influence = 0  # Influence score for tracking performance\n",
    "\n",
    "#     def choose_action(self, neighbors, epsilon=0.1):\n",
    "#         if random.uniform(0, 1) < epsilon or not neighbors:\n",
    "#             return random.choice(neighbors) if neighbors else None  # Explore\n",
    "#         state = tuple(neighbors)\n",
    "#         actions = self.qVals.get(state, {neighbor: 0 for neighbor in neighbors})\n",
    "#         return max(actions, key=actions.get)  # Exploit\n",
    "\n",
    "#     def update_qVal(self, state, action, reward, next_state, alpha=0.1, gamma=0.9):\n",
    "#         max_next_q = max(self.qVals.get(next_state, {}).values(), default=0)\n",
    "#         current_q = self.qVals.get(state, {}).get(action, 0)\n",
    "#         self.qVals.setdefault(state, {})[action] = current_q + alpha * (reward + gamma * max_next_q - current_q)\n",
    "\n",
    "\n",
    "# class SocialNetwork:\n",
    "#     def __init__(self, num_agents, agent_distribution):\n",
    "#         self.graph = nx.Graph()\n",
    "#         self.agents = []\n",
    "\n",
    "#         for i in range(num_agents):\n",
    "#             agent_type = random.choices(list(agent_distribution.keys()), weights=list(agent_distribution.values()))[0]\n",
    "#             agent = Agent(agent_type)\n",
    "#             self.graph.add_node(i, agent=agent)\n",
    "#             self.agents.append(agent)\n",
    "\n",
    "#         # Randomly connect agents\n",
    "#         for _ in range(num_agents * 2):  # Adjust for graph density\n",
    "#             a, b = random.sample(range(num_agents), 2)\n",
    "#             if not self.graph.has_edge(a, b):\n",
    "#                 self.graph.add_edge(a, b, weight=random.uniform(0.1, 1.0))\n",
    "\n",
    "#     def run_iteration(self, epsilon=0.1, alpha=0.1, gamma=0.9):\n",
    "#         for node, data in self.graph.nodes(data=True):\n",
    "#             agent = data['agent']\n",
    "#             neighbors = list(self.graph.neighbors(node))\n",
    "#             if not neighbors:\n",
    "#                 continue\n",
    "\n",
    "#             state = tuple(neighbors)\n",
    "#             action = agent.choose_action(neighbors, epsilon)\n",
    "\n",
    "#             # Simulate the action\n",
    "#             reward, next_state = self.simulate_action(node, action)\n",
    "#             agent.update_qVal(state, action, reward, next_state, alpha, gamma)\n",
    "\n",
    "#     def simulate_action(self, node, action):\n",
    "#         agent = self.graph.nodes[node]['agent']\n",
    "#         target_agent = self.graph.nodes[action]['agent']\n",
    "\n",
    "#         # Determine rewards based on agent types\n",
    "#         if agent.type == \"fake-information\":\n",
    "#             if target_agent.type == \"fact-checker\":\n",
    "#                 # Fake-information agent penalized if caught\n",
    "#                 reward = -1 if random.random() < 0.5 else 1\n",
    "#             else:\n",
    "#                 reward = 1  # Influence increased if news spreads\n",
    "#             agent.influence += reward\n",
    "\n",
    "#         elif agent.type == \"real-information\":\n",
    "#             reward = 1  # Rewarded for spreading truthful information\n",
    "#             agent.influence += reward\n",
    "\n",
    "#         elif agent.type == \"fact-checker\":\n",
    "#             if target_agent.type == \"fake-information\":\n",
    "#                 reward = 1  # Reward for catching fake news\n",
    "#             else:\n",
    "#                 reward = -1  # Penalized for mislabeling\n",
    "#             agent.influence += reward\n",
    "\n",
    "#         else:  # Regular agents\n",
    "#             reward = 0  # No reward or penalty for regular agents\n",
    "\n",
    "#         # Update target agent trust level\n",
    "#         trust_delta = 0.1 if reward > 0 else -0.1\n",
    "#         target_agent.trustLevel = max(0, min(1, target_agent.trustLevel + trust_delta))\n",
    "\n",
    "#         # New state is the updated neighbors list\n",
    "#         next_state = tuple(self.graph.neighbors(node))\n",
    "#         return reward, next_state\n",
    "\n",
    "#     def evaluate(self):\n",
    "#         influence_scores = [agent.influence for agent in self.agents]\n",
    "#         return {\n",
    "#             \"average_influence\": np.mean(influence_scores),\n",
    "#             \"max_influence\": max(influence_scores),\n",
    "#             \"min_influence\": min(influence_scores),\n",
    "#         }\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# num_agents = 100\n",
    "# agent_distribution = {\n",
    "#     \"real-information\": 0.3,\n",
    "#     \"fake-information\": 0.3,\n",
    "#     \"fact-checker\": 0.2,\n",
    "#     \"regular\": 0.2,\n",
    "# }\n",
    "\n",
    "# network = SocialNetwork(num_agents, agent_distribution)\n",
    "\n",
    "# # Run 100 iterations\n",
    "# for iteration in range(1000):\n",
    "#     network.run_iteration()\n",
    "\n",
    "# # Evaluate results\n",
    "# results = network.evaluate()\n",
    "# print(\"Simulation Results:\")\n",
    "# print(f\"Average Influence: {results['average_influence']:.2f}\")\n",
    "# print(f\"Max Influence: {results['max_influence']:.2f}\")\n",
    "# print(f\"Min Influence: {results['min_influence']:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
